{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:03:34.829788Z",
     "start_time": "2020-03-15T21:03:34.254220Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:03:36.087519Z",
     "start_time": "2020-03-15T21:03:35.933791Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cardio_train.csv\", delimiter=\";\", index_col=0)\n",
    "df.head()\n",
    "\n",
    "pcs = MinMaxScaler()\n",
    "variaveis_continuas = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"cholesterol\"]\n",
    "df[variaveis_continuas] = pcs.fit(df[variaveis_continuas]).transform(df[variaveis_continuas])\n",
    "\n",
    "df.gender = df.gender.apply(lambda genero: 0 if genero == 2 else genero)\n",
    "\n",
    "x, y = df.iloc[:,:-1].to_numpy(), df.iloc[:,-1].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.reshape((1, X_train.shape[1]))\n",
    "y_test = y_test.reshape((1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cada layer terá uma matriz de pesos W e um vetor bias b associado. A dimensão da matriz W é definida com a quantidade de linhas sendo igual a quantidade de nodos no layer e a quantidade colunas como a quantidade de inputs do layer. O vetor bias terá um bias para cada nodo layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:03:42.218364Z",
     "start_time": "2020-03-15T21:03:42.209816Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_parametros_iniciais(tamanho_layers):\n",
    "    np.random.seed(1)\n",
    "    parametros = []\n",
    "    for indice in range(1,len(tamanho_layers)):\n",
    "        parametros.append([np.random.randn(tamanho_layers[indice], tamanho_layers[indice-1]) * (2/np.sqrt(tamanho_layers[indice-1])),\n",
    "                           np.zeros((tamanho_layers[indice], 1))])\n",
    "    return parametros\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição das funções de ativação utilizadas: sigmóide e relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:03:43.662268Z",
     "start_time": "2020-03-15T21:03:43.657844Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    return A\n",
    "\n",
    "def softsign(Z):\n",
    "    A = np.divide(Z, (1+np.abs(Z)))\n",
    "    return A\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T18:47:53.154687Z",
     "start_time": "2020-03-11T18:47:53.149470Z"
    }
   },
   "source": [
    "### Esquema forward propagation\n",
    "<img src=\"imagens/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:06:19.249605Z",
     "start_time": "2020-03-15T21:06:19.242686Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parametros):\n",
    "    cache = []\n",
    "    A = X\n",
    "    quantidade_layers = len(parametros)\n",
    "    for indice_layer in range(0, quantidade_layers-1): # foward propagation até o ultimo layer antes do layer output\n",
    "        W = parametros[indice_layer][0]\n",
    "        b = parametros[indice_layer][1]\n",
    "        Z = np.dot(W, A) + b\n",
    "        cache.append((A, Z, W, b))\n",
    "        A = relu(Z)\n",
    "    W, b = parametros[indice_layer+1] # como é um problema de classificao, o último layer deve obrigatoriamente ter\n",
    "                                      #  a funcao sigmoide como funcao de ativação\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache.append((A, Z, W, b))\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A função custo usada aqui é definida como: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:06:46.096622Z",
     "start_time": "2020-03-15T21:06:46.092448Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_custo(A, Y, lambd, parametros):    \n",
    "    m = Y.shape[1]\n",
    "#     if 0 in A or 1 in a:\n",
    "#         print(\"zerro\")\n",
    "    l2 = lambd/(2*m) * np.sum([np.sum(np.square(w)) for w in parametros[:][0]])\n",
    "    custo = (1./m) * (-np.dot(Y,np.log(A).T) - np.dot(1-Y, np.log(1-A).T)) + l2\n",
    "    custo = float(np.squeeze(custo))\n",
    "    return custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:06:21.636295Z",
     "start_time": "2020-03-15T21:06:21.630101Z"
    }
   },
   "outputs": [],
   "source": [
    "def derivada_relu(dA, Z):\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def derivada_softsign(dA, Z):\n",
    "    return np.multiply(dA, np.divide(1, np.multiply(1+Z, 1+Z)))\n",
    "\n",
    "def derivada_sigmoide(dA, Z):\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:06:22.749736Z",
     "start_time": "2020-03-15T21:06:22.740793Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_propagation(A, cache, Y):\n",
    "    #print(A.shape)\n",
    "    m = A.shape[1]\n",
    "    Y = Y.reshape(A.shape)\n",
    "    gradientes = []\n",
    "    \n",
    "    dA = - (np.divide(Y, A) - np.divide(1 - Y, 1 - A)) # derivada do custo em função de A\n",
    "    A_prev, Z, W, b = cache[-1]\n",
    "    dZ = derivada_sigmoide(dA, Z)\n",
    "\n",
    "    dA_layer_anterior = np.dot(W.T, dZ)\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    gradientes.append([dW, db])\n",
    "\n",
    "    for cache_ in cache[::-1][1:]:\n",
    "        dA = dA_layer_anterior\n",
    "        A_prev, Z, W, b = cache_\n",
    "        dZ = derivada_relu(dA, Z)\n",
    "\n",
    "        dA_layer_anterior = np.dot(W.T, dZ)\n",
    "        dW = np.dot(dZ, A_prev.T) / m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        gradientes.append([dW, db])\n",
    "        \n",
    "    return gradientes[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:06:23.766075Z",
     "start_time": "2020-03-15T21:06:23.760341Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_parametros_atualizados(parametros, gradientes, m, lambd, taxa_aprendizado=0.05):\n",
    "    L = len(parametros) \n",
    "    for l in range(L):\n",
    "        parametros[l][0] -= taxa_aprendizado * (gradientes[l][0] + lambd/m * parametros[l][0])\n",
    "        parametros[l][1] -= taxa_aprendizado * gradientes[l][1] \n",
    "    return parametros\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:17:14.918398Z",
     "start_time": "2020-03-15T21:17:14.911133Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(X, Y, taxa_aprendizado, iteracoes, tamanhos_layers, lambd=0.7):\n",
    "    global gradientes\n",
    "    global parametros\n",
    "    parametros = get_parametros_iniciais(tamanhos_layers)\n",
    "    custos = []\n",
    "    for iteracao in range(iteracoes):\n",
    "        A, cache = forward_propagation(X, parametros)\n",
    "        custo = get_custo(A, Y, lambd, parametros)\n",
    "        custos.append(custo)\n",
    "        if iteracao % 1000 == 0:\n",
    "            print(\"Iteração: {} | Custo: {}\".format(iteracao, custo))\n",
    "        gradientes = backward_propagation(A, cache, Y)\n",
    "        parametros = get_parametros_atualizados(parametros, gradientes, X.shape[1], lambd, taxa_aprendizado)\n",
    "    \n",
    "    return parametros, custos        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:14:54.832151Z",
     "start_time": "2020-03-15T21:14:54.827452Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, Y, parametros):\n",
    "    A, cache = forward_propagation(X, parametros)\n",
    "    return np.round(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:06:26.149546Z",
     "start_time": "2020-03-15T21:06:26.146801Z"
    }
   },
   "outputs": [],
   "source": [
    "# def converte_array_em_parametros(array, tamanho_layers):\n",
    "#     parametros = []\n",
    "#     for index in range(1, len(tamanho_layers)):\n",
    "#         w_shape_linear = tamanho_layers[index]*tamanho_layers[index-1]\n",
    "#         w = array[:w_shape_linear].reshape((tamanho_layers[index], tamanho_layers[index-1]))\n",
    "#         b = array[w_shape_linear:w_shape_linear + tamanho_layers[index]].reshape((tamanho_layers[index], 1))\n",
    "#         parametros.append([w, b])\n",
    "#         array = array[w_shape_linear + tamanho_layers[index]:]\n",
    "#     return parametros\n",
    "\n",
    "# def verifica_backpropagation(parametros, gradientes, tamanho_layers, X, Y, epsilon=1e-7):\n",
    "#     parameters_values = np.concatenate([np.concatenate([parametro[0].flatten(), parametro[1].flatten()]) \n",
    "#                                        for parametro in parametros])\n",
    "#     grads =  np.concatenate([np.concatenate([gradiente[0].flatten(), gradiente[1].flatten()]) \n",
    "#                                        for gradiente in gradientes])\n",
    "    \n",
    "#     num_parameters = parameters_values.shape[0]\n",
    "#     J_plus = np.zeros((num_parameters, 1))\n",
    "#     J_minus = np.zeros((num_parameters, 1))\n",
    "#     gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "#     for i in range(num_parameters):\n",
    "        \n",
    "#         # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "#         # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "#         ### START CODE HERE ### (approx. 3 lines)\n",
    "#         thetaplus =  np.copy(parameters_values)                                       # Step 1\n",
    "#         thetaplus[i] = thetaplus[i] + epsilon                                   # Step 2\n",
    "#         A, _ =  forward_propagation(X, converte_array_em_parametros(thetaplus, tamanho_layers))  # Step 3\n",
    "#         J_plus[i][0] = get_custo(A, Y, 0.7, converte_array_em_parametros(thetaplus, tamanho_layers))\n",
    "#         ### END CODE HERE ###\n",
    "        \n",
    "#         # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "#         ### START CODE HERE ### (approx. 3 lines)\n",
    "#         thetaminus = np.copy(parameters_values)                                       # Step 1\n",
    "#         thetaminus[i] = thetaminus[i] - epsilon                                 # Step 2       \n",
    "#         A, _ = forward_propagation(X, converte_array_em_parametros(thetaminus, tamanho_layers)) # Step 3\n",
    "#         J_minus[i][0] = get_custo(A, Y, 0.7, converte_array_em_parametros(thetaminus, tamanho_layers))\n",
    "#         ### END CODE HERE ###\n",
    "        \n",
    "#         # Compute gradapprox[i]\n",
    "#         ### START CODE HERE ### (approx. 1 line)\n",
    "#         gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "#         ### END CODE HERE ###\n",
    "    \n",
    "#     # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "#     ### START CODE HERE ### (approx. 1 line)\n",
    "#     numerator = np.linalg.norm(grads - gradapprox)                                     # Step 1'\n",
    "#     denominator = np.linalg.norm(grads) + np.linalg.norm(gradapprox)                   # Step 2'\n",
    "#     difference = numerator / denominator                                              # Step 3'\n",
    "#     ### END CODE HERE ###\n",
    "\n",
    "#     if difference > 1e-7:\n",
    "#         print(\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "#     else:\n",
    "#         print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "#     return difference\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-15T21:41:43.559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração: 0 | Custo: 1.3531599175681845\n",
      "Iteração: 1000 | Custo: 0.6401567957135242\n",
      "Iteração: 2000 | Custo: 0.6327863026034765\n",
      "Iteração: 3000 | Custo: 0.6318209809444724\n",
      "Iteração: 4000 | Custo: 0.6305476762910055\n",
      "Iteração: 5000 | Custo: 0.6301878129691637\n",
      "Iteração: 6000 | Custo: 0.6302138327016902\n",
      "Iteração: 7000 | Custo: 0.6294049887037406\n",
      "Iteração: 8000 | Custo: 0.629028232682224\n",
      "Iteração: 9000 | Custo: 0.6288087594502474\n",
      "Iteração: 10000 | Custo: 0.6284439292547017\n",
      "Iteração: 11000 | Custo: 0.6282097184136229\n",
      "Iteração: 12000 | Custo: 0.6278294947851617\n",
      "Iteração: 13000 | Custo: 0.6278970331056625\n",
      "Iteração: 14000 | Custo: 0.6282035039561675\n",
      "Iteração: 15000 | Custo: 0.6274104317066351\n",
      "Iteração: 16000 | Custo: 0.6271447926073501\n",
      "Iteração: 17000 | Custo: 0.6274893735204532\n"
     ]
    }
   ],
   "source": [
    "tamanho_layers = [11, 25, 5, 3, 1]\n",
    "gradientes = []\n",
    "parametros = []\n",
    "parametros, custos = fit(X_train, y_train, 0.05, 50000, tamanho_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:36:08.358848Z",
     "start_time": "2020-03-15T21:36:08.346888Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = predict(X_train, y_train, parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:36:09.019814Z",
     "start_time": "2020-03-15T21:36:08.968649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6066019571646917"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train[0], train_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:36:10.024594Z",
     "start_time": "2020-03-15T21:36:10.019901Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pred = predict(X_test, y_test, parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T21:36:10.463332Z",
     "start_time": "2020-03-15T21:36:10.429909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6005085015096139"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test[0], test_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
